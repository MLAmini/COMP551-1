{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parse covariance matrix data\n",
    "cov = pandas.read_csv('./dataset/DS1_Cov.txt', header=None)\n",
    "cov.drop([20], axis=1, inplace=True)\n",
    "cov = cov.values\n",
    "\n",
    "#Parse the mean vectors\n",
    "m0 = numpy.genfromtxt('./dataset/DS1_m_0.txt', delimiter=',')[:-1]\n",
    "m1 = numpy.genfromtxt('./dataset/DS1_m_1.txt', delimiter=',')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds1 = []\n",
    "ds0 = []\n",
    "for i in range(2000):\n",
    "    ds1.append(numpy.random.multivariate_normal(m1, cov))\n",
    "    ds0.append(numpy.random.multivariate_normal(m0, cov))\n",
    "ds1 = pandas.DataFrame(ds1)\n",
    "ds0 = pandas.DataFrame(ds0)\n",
    "ds0[20] = 0\n",
    "ds1[20] = 1\n",
    "\n",
    "msk = numpy.random.rand(len(ds1)) <= 0.7\n",
    "train1 = ds1[msk]\n",
    "test1 = ds1[~msk]\n",
    "\n",
    "msk = numpy.random.rand(len(ds0)) <= 0.7\n",
    "train0 = ds0[msk]\n",
    "test0 = ds0[~msk]\n",
    "\n",
    "train = pandas.concat([train0, train1], ignore_index=True)\n",
    "test = pandas.concat([test0, test1], ignore_index=True)\n",
    "\n",
    "train1.to_csv('DS1_train1.csv')\n",
    "test1.to_csv('DS1_test1.csv')\n",
    "train0.to_csv('DS1_train0.csv')\n",
    "test0.to_csv('DS1_test0.csv')\n",
    "\n",
    "test.to_csv('DS1_test.csv')\n",
    "train.to_csv('DS1_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len0 = len(train0)\n",
    "len1 = len(train1)\n",
    "\n",
    "\n",
    "# drop the output column\n",
    "train0 = train[train[20] == 0]\n",
    "train0 = train0[train0.columns.difference([20])]\n",
    "\n",
    "train1 = train[train[20] == 1]\n",
    "train1 = train1[train1.columns.difference([20])]\n",
    "\n",
    "test_output = test[20]\n",
    "test.drop([20], axis=1, inplace=True)\n",
    "\n",
    "train_output = train[20]\n",
    "train.drop([20], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0:  27.8476877165\n",
      "w1: [14.707729156895715, -8.8305406001678168, -5.5221547556501562, -2.6214071915318229, -10.02977954152756, -4.1847592094945387, 16.79210358829441, -25.039403079155537, -29.697412075196127, 9.5566884645144761, -13.382001513262969, -12.230248062200609, 15.742262171088065, 13.102390147524336, -5.9258220337026728, 13.480181549007739, 29.549738076362232, -6.9738799729481098, 0.013707430910422147, -5.3335266414797884]\n",
      "\n",
      "[[ -1.00000000e+00   0.00000000e+00   1.00000000e+00]\n",
      " [  2.60000000e+01   1.16300000e+03   2.70000000e+01]]\n",
      "True positive: 579.0\n",
      "True negative: 584.0\n",
      "\n",
      "F measure: 0.956234516928\n",
      "Accuracy: 0.956414473684\n",
      "Precision: 0.955445544554\n",
      "Recall: 0.957024793388\n"
     ]
    }
   ],
   "source": [
    "# estimated max probability\n",
    "prob_0 = float(len0) / float(len0 + len1)\n",
    "prob_1 = 1.0 - prob_0\n",
    "\n",
    "\n",
    "mean_0 = numpy.array(train0.mean())\n",
    "mean_1 = numpy.array(train1.mean())\n",
    "\n",
    "diff_0 = numpy.array(train0 - mean_0)\n",
    "diff_1 = numpy.array(train1 - mean_1)\n",
    "\n",
    "coeff_0 = numpy.matmul(diff_0.T, diff_0)\n",
    "coeff_1 = numpy.matmul(diff_1.T, diff_1)\n",
    "\n",
    "coeff = (coeff_0 + coeff_1) / float(len0 + len1)\n",
    "\n",
    "covm_0 = numpy.matmul(numpy.linalg.pinv(coeff), mean_0)\n",
    "mcovm_0 = numpy.matmul(mean_0.T, covm_0)\n",
    "\n",
    "covm_1 = numpy.matmul(numpy.linalg.pinv(coeff), mean_1)\n",
    "mcovm_1 = numpy.matmul(mean_1.T, covm_1)\n",
    "\n",
    "w0 = math.log(prob_0) - math.log(prob_1) - 1 / 2 * (mcovm_0 - mcovm_1)\n",
    "w1 = numpy.matmul(numpy.linalg.pinv(coeff), mean_0 - mean_1)\n",
    "\n",
    "\n",
    "print(\"w0: \", w0)\n",
    "print(\"w1: \" + str([i for i in w1]) + \"\\n\")\n",
    "\n",
    "pred = numpy.matmul(test, w1) + w0\n",
    "\n",
    "# replace dec_bound to 0 and 1\n",
    "pred[pred > 0] = 0\n",
    "pred[pred < 0] = 1\n",
    "\n",
    "error_matrix = pred - test_output\n",
    "\n",
    "elem, cnt = numpy.unique(error_matrix, return_counts=True)\n",
    "\n",
    "print(numpy.asarray([elem, cnt]))\n",
    "\n",
    "'''\n",
    "1 => false positive\n",
    "0 => correct prediction\n",
    "-1 => false negative\n",
    "'''\n",
    "\n",
    "fp = cnt[2]\n",
    "fn = cnt[0]\n",
    "\n",
    "\n",
    "tp = numpy.sum(pred.dot(test_output))\n",
    "print(\"True positive: \" + str(tp))\n",
    "\n",
    "pred = pred - 1\n",
    "test_output_temp = test_output - 1\n",
    "tn = numpy.sum(pred.dot(test_output_temp))\n",
    "print(\"True negative: \" + str(tn))\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "\n",
    "F = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"\\nF measure: \" + str(F))\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per k:  [0.51315789473684215, 0.51233552631578949, 0.54523026315789469, 0.52302631578947367, 0.52138157894736847, 0.52467105263157898, 0.52878289473684215, 0.53782894736842102, 0.53700657894736847, 0.53125, 0.52549342105263153, 0.51644736842105265, 0.52796052631578949, 0.53289473684210531, 0.52713815789473684, 0.52467105263157898, 0.54440789473684215, 0.53371710526315785, 0.53700657894736847, 0.52384868421052633, 0.53207236842105265, 0.54194078947368418, 0.54605263157894735, 0.53947368421052633, 0.55263157894736847, 0.54934210526315785, 0.54769736842105265, 0.55098684210526316, 0.53947368421052633, 0.54605263157894735, 0.55674342105263153, 0.55345394736842102, 0.55427631578947367, 0.56003289473684215, 0.55592105263157898, 0.56003289473684215, 0.55345394736842102, 0.55838815789473684, 0.5625, 0.5625, 0.5625, 0.5625, 0.56085526315789469, 0.57072368421052633, 0.56085526315789469, 0.57072368421052633, 0.57319078947368418, 0.56825657894736847, 0.57154605263157898]\n",
      "\n",
      "Best k = 47\n",
      "\n",
      "Accuracy = 0.573190789474\n",
      "Precision = 0.6363636363636364\n",
      "Recall = 0.5628654970760234\n",
      "F - measure = 0.5973622963537627\n"
     ]
    }
   ],
   "source": [
    "res_step = []\n",
    "accuracy_k = []\n",
    "\n",
    "\n",
    "for k in range(1, 50):\n",
    "    # metrics = [tp, tn, fp, fn]\n",
    "    metrics = [0, 0, 0, 0]\n",
    "    for i in range(len(test)):\n",
    "        dist = abs(train.sub(numpy.array(numpy.array(test.loc[[i], :])[0])))\n",
    "        dist = numpy.array(numpy.power(dist, 2).sum(axis=1))\n",
    "\n",
    "        # select the k training examples closest to the test example\n",
    "        ind = numpy.argpartition(dist, k)[:k]\n",
    "\n",
    "        # predict value of test output using the average of the k training outputs\n",
    "        dec = 1 if (numpy.array([train_output[i] for i in ind]).mean() > 0.5) else 0\n",
    "\n",
    "        #update metrics\n",
    "        if dec == test_output[i] == 1:\n",
    "            metrics[0] += 1\n",
    "        elif dec == test_output[i] == 0:\n",
    "            metrics[1] += 1\n",
    "        elif dec != test_output[i] == 1:\n",
    "            metrics[2] += 1\n",
    "        else:\n",
    "            metrics[3] += 1\n",
    "\n",
    "    res_step.append(metrics)\n",
    "    accuracy_k.append((metrics[0] + metrics[1]) / numpy.sum(metrics))\n",
    "\n",
    "\n",
    "print(\"Accuracy per k: \", accuracy_k)\n",
    "\n",
    "print(\"\\nBest k = \" + str(accuracy_step.index(max(accuracy_k)) + 1))\n",
    "\n",
    "metrics = res_step[accuracy_k.index(max(accuracy_k))]\n",
    "tp = metrics[0]\n",
    "tn = metrics[1]\n",
    "fp = metrics[2]\n",
    "fn = metrics[3]\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tn + tp) / (tn + tp + fn + fp)\n",
    "\n",
    "F = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"\\nAccuracy = \" + str(max(accuracy_step)))\n",
    "print(\"Precision = \" + str(precision))\n",
    "print(\"Recall = \" + str(recall))\n",
    "print(\"F - measure = \" + str(F))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
